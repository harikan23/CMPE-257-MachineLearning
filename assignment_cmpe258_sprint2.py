# -*- coding: utf-8 -*-
"""Assignment-CMPE258-Sprint2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nWSnmU7aO5fnX1GconlB7Ar6kn1gnSu4

Alternus Vera Sprint 2

Submitted by Harika Nalam
Team Name: The Trio

Main Factor - News Coverage
Micro Factors - Business, Reports, Book Reviews, Broadcast

Problem Statement  - Given an article classifiy the text to a particular. Score is calculated which gives the coverage of the article to a particular topic.    

LDA:   
LDA is used to classify text in a document in aparticular topic using the score, the score the probability distribution of the words.    

DataSet1 - [link](https://components.one/datasets/all-the-news-articles-dataset/)
"""

import pandas as pd
import sqlite3
import seaborn as sns
from nltk import sent_tokenize

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
np.random.seed(400)

import nltk
nltk.download('wordnet')

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

db = '/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/dataset/all-the-news.db'

cnx = sqlite3.connect(db)

df = pd.read_sql_query("SELECT * FROM longform", cnx)
df.head()

train = df.sample(frac = 0.7)
test = df.drop(train.index)

df=train

"""Tokenization and Lemmatize

Data Cleaning:
1. Tokenization: split the text into and sentences into words. Lowercase the words and remove punctuation
2. Lemmatized: Words in third person are changed to first person and verbs in past and furture tense are changed into present
"""

stemmer = SnowballStemmer("english")
def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def preprocess(text):
    result=[]
    for token in gensim.utils.simple_preprocess(text) :
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return result

df['category'].unique()

"""group data by category"""

data_group = df.groupby('category')
# data_group.head(5)

"""And the categories I'm considering are 
1. Business
2. Boradcast
3. Book Reviews
4. Reports
"""

data_business = data_group.get_group('business')
data_broadcast = data_group.get_group('broadcast')
data_BookReviews = data_group.get_group('Book Reviews')
data_Reports = data_group.get_group('Reports')

"""##Business Data"""

data_business = data_business[:300000][['title']]
data_business['index'] = data_business.index
documents_business = data_business

"""Tokenize the document"""

documents_business['title'][56128]

document_num = 56128
doc_sample = documents_business[documents_business['index'] == document_num].values[0][0]

print("Original document: ")
words = []
for word in doc_sample.split(' '):
    words.append(word)
print(words)
print("\n\nTokenized and lemmatized document: ")
print(preprocess(doc_sample))

documents_business.head()

path = '/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/dataset/BusinessArticles.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  documents_business.to_csv(f)

processed_docs_business = documents_business['title'].map(preprocess)
processed_docs_business.head()

"""Bag of words:    
Create a dictionary containing the number of times a word appears in the training set. 
Pass the documents to gensim.corpora.Dictionary()
"""

dictionary_business = gensim.corpora.Dictionary(processed_docs_business)
count = 0
for k, v in dictionary_business.iteritems():
    print(k, v)
    count += 1
    if count > 5:
        break

import pickle 

with open("/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/dictionary_business.pkl", "wb") as pkl_handle:
	pickle.dump(dictionary_business, pkl_handle)



"""Gensim doc2bow:   
convert document into the bag-of-words formart = list(word_id, word,count) 
"""

bow_corpus_business = [dictionary_business.doc2bow(doc) for doc in processed_docs_business]

"""TF-IDF: Term Frequency, Inverse Document Frequency.    
1. TFIDF is used to score the importance of words in a document based on their frequency across multiple documents
2. If words appears frequenctly in a document is important and given high score
3. words appear in many document are not considered unique, low score for sucnh words. like the , for


Example: TF IDF
document d1 - 1000 words and word "stock" appears 5 times
TF = 5/1000 = 0.005

10 million documents word "stock" appears 1000 times then
IDF  = log(10000000/1000)= 4

TF-IDF = 0.005 *4 = 0.02

"""

from gensim import corpora, models
tfidf = models.TfidfModel(bow_corpus_business)
corpus_tfidf_business = tfidf[bow_corpus_business]
from pprint import pprint
for doc in corpus_tfidf_business:
    pprint(doc)
    break

"""Running LDA using Bag of words




"""

lda_model_business = gensim.models.LdaMulticore(bow_corpus_business, 
                                       num_topics=10, 
                                       id2word = dictionary_business, 
                                       passes = 2, 
                                       workers=2)

for idx, topic in lda_model_business.print_topics(-1):
    print("Topic: {} \nWords: {}".format(topic, idx ))
    print("\n")

"""Running LDA using TF-IDF"""

lda_model_tfidf_business = gensim.models.LdaMulticore(corpus_tfidf_business, 
                                       num_topics=10, 
                                       id2word = dictionary_business, 
                                       passes = 2, 
                                       workers=2)

for idx, topic in lda_model_tfidf_business.print_topics(-1):
    print("Topic: {} Word: {}".format(idx, topic))
    print("\n")

"""Save Models"""

lda_model_tfidf_business.save('/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/LDA_TFID_Business.sav')
lda_model_business.save('/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/lda_model_business.sav')

unseen_document = "Dow tumbles more than 900 points, Nasdaq drops 4 percent to close out brutal month"

# Data preprocessing step for the unseen document
bow_vector = dictionary_business.doc2bow(preprocess(unseen_document))

for index, score in sorted(lda_model_business[bow_vector], key=lambda tup: -1*tup[1]): 
    print("Score: {}\t Topic: {}".format(score, lda_model_business.print_topic(index, 5)))

"""##Broadcast"""

data_broadcast = data_broadcast[:300000][['title']]
data_broadcast['index'] = data_broadcast.index
documents_broadcast = data_broadcast

documents_broadcast

documents_broadcast['title'][41840]

document_num = 41840
doc_sample = documents_broadcast[documents_broadcast['index'] == document_num].values[0][0]

print("Original document: ")
words = []
for word in doc_sample.split(' '):
    words.append(word)
print(words)
print("\n\nTokenized and lemmatized document: ")
print(preprocess(doc_sample))

path = '/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/dataset/BroadcastArticles.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  documents_broadcast.to_csv(f)

processed_docs_broadcast = documents_broadcast['title'].map(preprocess)
processed_docs_broadcast.head()

"""Bag of words"""

dictionary_broadcast = gensim.corpora.Dictionary(processed_docs_broadcast)
count = 0
for k, v in dictionary_broadcast.iteritems():
    print(k, v)
    count += 1
    if count > 5:
        break

"""Gensim doc2bow"""

bow_corpus_broadcast = [dictionary_broadcast.doc2bow(doc) for doc in processed_docs_broadcast]

"""TF-IDF"""

tfidf = models.TfidfModel(bow_corpus_broadcast)
corpus_tfidf_broadcast = tfidf[bow_corpus_broadcast]
from pprint import pprint
for doc in corpus_tfidf_broadcast:
    pprint(doc)
    break

"""Running LDA model using Bag of words"""

lda_model_broadcast = gensim.models.LdaMulticore(bow_corpus_broadcast, 
                                       num_topics=10, 
                                       id2word = dictionary_broadcast, 
                                       passes = 2, 
                                       workers=2)

for idx, topic in lda_model_broadcast.print_topics(-1):
    print("Topic: {} \nWords: {}".format(topic, idx ))
    print("\n")

"""Running LDA using TF-IDF"""

lda_model_tfidf_broadcast = gensim.models.LdaMulticore(corpus_tfidf_broadcast, 
                                       num_topics=10, 
                                       id2word = dictionary_broadcast, 
                                       passes = 2, 
                                       workers=2)

for idx, topic in lda_model_tfidf_broadcast.print_topics(-1):
    print("Topic: {} Word: {}".format(idx, topic))
    print("\n")

"""Save models"""

lda_model_tfidf_broadcast.save('/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/LDA_TFID_broadcast.sav')
lda_model_broadcast.save('/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/lda_model_broadcast.sav')

unseen_document = "Country music legend Naomi Judd, one half of The Judds, dies age 76"

# Data preprocessing step for the unseen document
bow_vector = dictionary_broadcast.doc2bow(preprocess(unseen_document))

for index, score in sorted(lda_model_broadcast[bow_vector], key=lambda tup: -1*tup[1]): 
    print("Score: {}\t Topic: {}".format(score, lda_model_broadcast.print_topic(index, 5)))

"""##Book Reviews"""

data_BookReviews = data_BookReviews[:300000][['title']]
data_BookReviews['index'] = data_BookReviews.index
documents_BookReviews = data_BookReviews

documents_BookReviews['title'][1213]

document_num = 1213
doc_sample = documents_BookReviews[documents_BookReviews['index'] == document_num].values[0][0]

print("Original document: ")
words = []
for word in doc_sample.split(' '):
    words.append(word)
print(words)
print("\n\nTokenized and lemmatized document: ")
print(preprocess(doc_sample))

path = '/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/dataset/BookReview.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  documents_BookReviews.to_csv(f)

processed_docs_BookReviews = documents_BookReviews['title'].map(preprocess)
processed_docs_BookReviews.head()

"""Bag of words"""

dictionary_BookReviews = gensim.corpora.Dictionary(processed_docs_BookReviews)
count = 0
for k, v in dictionary_BookReviews.iteritems():
    print(k, v)
    count += 1
    if count > 5:
        break

"""Gensim doc2bow"""

bow_corpus_BookReviews = [dictionary_BookReviews.doc2bow(doc) for doc in processed_docs_BookReviews]

"""TF - IDF"""

tfidf = models.TfidfModel(bow_corpus_BookReviews)
corpus_tfidf_BookReviews = tfidf[bow_corpus_BookReviews]
from pprint import pprint
for doc in corpus_tfidf_BookReviews:
    pprint(doc)
    break

"""Running LDA using Bag of words"""

lda_model_BookReviews = gensim.models.LdaMulticore(bow_corpus_BookReviews, 
                                       num_topics=10, 
                                       id2word = dictionary_BookReviews, 
                                       passes = 2, 
                                       workers=2)

for idx, topic in lda_model_BookReviews.print_topics(-1):
    print("Topic: {} \nWords: {}".format(topic, idx ))
    print("\n")

"""Running LDA using TF IDF"""

lda_model_tfidf_BookReviews = gensim.models.LdaMulticore(corpus_tfidf_BookReviews, 
                                       num_topics=10, 
                                       id2word = dictionary_BookReviews, 
                                       passes = 2, 
                                       workers=2)

for idx, topic in lda_model_tfidf_BookReviews.print_topics(-1):
    print("Topic: {} \nWords: {}".format(topic, idx ))
    print("\n")

lda_model_tfidf_BookReviews.save('/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/LDA_TFID_bookreview.sav')
lda_model_BookReviews.save('/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/lda_model_bookreview.sav')

unseen_document = "At 100, the ‘Just William’ Books Are an Icon of British Childhood"

# Data preprocessing step for the unseen document
bow_vector = dictionary_BookReviews.doc2bow(preprocess(unseen_document))

for index, score in sorted(lda_model_BookReviews[bow_vector], key=lambda tup: -1*tup[1]): 
    print("Score: {}\t Topic: {}".format(score, lda_model_BookReviews.print_topic(index, 5)))

"""##Reprts"""

data_Reports = data_Reports[:300000][['title']]
data_Reports['index'] = data_Reports.index
documents_Reports = data_Reports

documents_Reports['title'][683]

document_num = 683
doc_sample = documents_Reports[documents_Reports['index'] == document_num].values[0][0]

print("Original document: ")
words = []
for word in doc_sample.split(' '):
    words.append(word)
print(words)
print("\n\nTokenized and lemmatized document: ")
print(preprocess(doc_sample))

path = '/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/dataset/reprtsData.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  documents_Reports.to_csv(f)

processed_docs_Reports = documents_Reports['title'].map(preprocess)
processed_docs_Reports.head()

dictionary_Reports = gensim.corpora.Dictionary(processed_docs_Reports)
count = 0
for k, v in dictionary_Reports.iteritems():
    print(k, v)
    count += 1
    if count > 5:
        break

"""Gensim doc2bow"""

bow_corpus_Reports = [dictionary_Reports.doc2bow(doc) for doc in processed_docs_Reports]

"""TF IDF"""

tfidf = models.TfidfModel(bow_corpus_Reports)
corpus_tfidf_Reports = tfidf[bow_corpus_Reports]
from pprint import pprint
for doc in corpus_tfidf_BookReviews:
    pprint(doc)
    break

"""Running LDA using bag of words"""

lda_model_Reports = gensim.models.LdaMulticore(bow_corpus_Reports, 
                                       num_topics=10, 
                                       id2word = dictionary_Reports, 
                                       passes = 2, 
                                       workers=2)

for idx, topic in lda_model_Reports.print_topics(-1):
    print("Topic: {} \nWords: {}".format(topic, idx ))
    print("\n")

"""Running LDA using TF IDF"""

lda_model_tfidf_Reports = gensim.models.LdaMulticore(corpus_tfidf_Reports, 
                                       num_topics=10, 
                                       id2word = dictionary_Reports, 
                                       passes = 2, 
                                       workers=2)

for idx, topic in lda_model_tfidf_Reports.print_topics(-1):
    print("Topic: {} \nWords: {}".format(topic, idx ))
    print("\n")

lda_model_tfidf_Reports.save('/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/LDA_TFID_reports.sav')
lda_model_Reports.save('/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/lda_model_reports.sav')

unseen_document = "Romance Between Business and the Republican Party Hits the Rocks"

# Data preprocessing step for the unseen document
bow_vector = dictionary_Reports.doc2bow(preprocess(unseen_document))

for index, score in sorted(lda_model_Reports[bow_vector], key=lambda tup: -1*tup[1]): 
    print("Score: {}\t Topic: {}".format(score, lda_model_Reports.print_topic(index, 5)))

"""##Test MicroFactor Models"""

factors = ['business', 'Reports', 'Book Reviews', 'broadcast']
test = test[test.category.isin(factors)]

test['category'].unique()

del test['author']
del test['content']
del test['year']
del test['month']
del test['digital']
del test['section']
del test['url']

del test['date']

test.head(5)

test['business_factor_score'] = 0
# test['business_factor_topic'] = "NAN"
# test['Reports_score'] = ""
# test['broadcast_score'] = ""
# test['BookReviews_score'] = ""
factor_score = []

for index, row in test.iterrows():
    title = row['title']
    # print(title)
    bow_vector = dictionary_business.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_business[bow_vector], key=lambda tup: -1*tup[1]): 
      row['business_factor_score'] = score
      factor_score.append(score)
      row['business_factor_topic'] = lda_model_business.print_topic(index, 5)
      break;

test['business_factor_score'] = factor_score

broadcast_score = []

for index, row in test.iterrows():
    title = row['title']
    bow_vector = dictionary_broadcast.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_broadcast[bow_vector], key=lambda tup: -1*tup[1]): 
      # row['broadcast_score'] = score
      broadcast_score.append(score)
      # print(row['broadcast_factor_score'])
      break;

test['broadcast_score'] = 0

test['broadcast_score'] = broadcast_score

BookReviews_score=[]
test['BookReviews_score'] = 0

for index, row in test.iterrows():
    title = row['title']
    bow_vector = dictionary_BookReviews.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_BookReviews[bow_vector], key=lambda tup: -1*tup[1]): 
      # row['BookReviews_score'] = score
      BookReviews_score.append(score)
      break;

test['BookReviews_score'] = BookReviews_score

report_factor_score=[]
test['report_factor_score'] = 0

for index, row in test.iterrows():
    title = row['title']
    bow_vector = dictionary_Reports.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_Reports[bow_vector], key=lambda tup: -1*tup[1]): 
      # row['report_factor_score'] = score
      # row['report_factor_topic'] = lda_model_Reports.print_topic(index, 5)
      report_factor_score.append(score)
      break;

test['report_factor_score'] = report_factor_score

test['category'].unique()

test.head()

error=0

for index, row in test.iterrows():
  if row['category']=="Reports":
    error+=(1-row['report_factor_score'])
  elif row['category']=="Book Reviews":
    error+=(1-row['BookReviews_score'])
  elif row['category']=="broadcast":
    error+=(1-row['broadcast_score'])
  elif row['category']=="business":
    error+=(1-row['business_factor_score'])

"""RMSE of the model"""

print("RMSE:",np.sqrt(error/test.shape[0]))

"""##Load new data and get the dataset to be classified into different microfactors"""

test_filename = '/content/drive/MyDrive/ML-Spring-2022/AlternusVera/test.tsv'
train_filename = '/content/drive/MyDrive/ML-Spring-2022/AlternusVera/train.tsv'
valid_filename = '/content/drive/MyDrive/ML-Spring-2022/AlternusVera/valid.tsv'

colnames = ['jsonid', 'label', 'headline_text', 'subject', 'speaker', 'speakerjobtitle', 'stateinfo','partyaffiliation', 'barelytruecounts', 'falsecounts','halftruecounts','mostlytrueocunts','pantsonfirecounts','context']

train_news = pd.read_csv(train_filename, sep='\t', names = colnames, error_bad_lines=False)
test_news = pd.read_csv(test_filename, sep='\t', names = colnames, error_bad_lines=False)
valid_news = pd.read_csv(valid_filename, sep='\t', names = colnames, error_bad_lines=False)

train_news.head(5)

# del train_news['speaker']
# del train_news['speakerjobtitle']
# del train_news['stateinfo']
# del train_news['partyaffiliation']
# del train_news['barelytruecounts']
# del train_news['falsecounts']
# del train_news['halftruecounts']
# del train_news['mostlytrueocunts']
# del train_news['pantsonfirecounts']

train_news.head(5)

train_news.shape

"""##Business Score to New Data

Adding topic and score of each micro factor to text in dataframe
"""

business_factor_score = []
train_news['business_factor_score'] = " "

for index, row in train_news.iterrows():
    title = row['headline_text']
    bow_vector = dictionary_business.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_business[bow_vector], key=lambda tup: -1*tup[1]): 
      # row['business_factor_score'] = score
      business_factor_score.append(score)
      # row['business_factor_topic'] = lda_model_business.print_topic(index, 5)
      break;

train_news['business_factor_score'] = business_factor_score

train_news

business_factor_score1 = []
test_news['business_factor_score'] = " "

for index, row in test_news.iterrows():
    title = row['headline_text']
    bow_vector = dictionary_business.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_business[bow_vector], key=lambda tup: -1*tup[1]): 
      # row['business_factor_score'] = score
      business_factor_score1.append(score)
      # row['business_factor_topic'] = lda_model_business.print_topic(index, 5)
      break;

test_news['business_factor_score'] = business_factor_score1

"""##Broadcast Factor score on New Data"""

broadcast_factor_score=[]
train_news['broadcast_factor_score'] = ""

for index, row in train_news.iterrows():
    title = row['headline_text']
    bow_vector = dictionary_broadcast.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_broadcast[bow_vector], key=lambda tup: -1*tup[1]): 
      # row['broadcast_factor_score'] = score
      broadcast_factor_score.append(score)
      # row['broadcast_factor_topic'] = lda_model_broadcast.print_topic(index, 5)
      # print(row['broadcast_factor_score'])
      break;

train_news['broadcast_factor_score'] = broadcast_factor_score

train_news.head(5)

broadcast_factor_score1=[]
test_news['broadcast_factor_score'] = ""

for index, row in test_news.iterrows():
    title = row['headline_text']
    bow_vector = dictionary_broadcast.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_broadcast[bow_vector], key=lambda tup: -1*tup[1]): 
      # row['broadcast_factor_score'] = score
      broadcast_factor_score1.append(score)
      # row['broadcast_factor_topic'] = lda_model_broadcast.print_topic(index, 5)
      # print(row['broadcast_factor_score'])
      break;

test_news['broadcast_factor_score'] = broadcast_factor_score1

"""##Book Review factor score for new Data"""

bookReview_factor_score=[]
train_news['bookReview_factor_score'] = ""

for index, row in train_news.iterrows():
    title = row['headline_text']
    bow_vector = dictionary_BookReviews.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_BookReviews[bow_vector], key=lambda tup: -1*tup[1]): 
      # row['bookReview_factor_score'] = score
      bookReview_factor_score.append(score)
      # row['bookReview_factor_topic'] = lda_model_BookReviews.print_topic(index, 5)
      break;

train_news['bookReview_factor_score'] = bookReview_factor_score

train_news.head(5)

bookReview_factor_score1=[]
test_news['bookReview_factor_score'] = ""

for index, row in test_news.iterrows():
    title = row['headline_text']
    bow_vector = dictionary_BookReviews.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_BookReviews[bow_vector], key=lambda tup: -1*tup[1]): 
      # row['bookReview_factor_score'] = score
      bookReview_factor_score1.append(score)
      # row['bookReview_factor_topic'] = lda_model_BookReviews.print_topic(index, 5)
      break;
  
test_news['bookReview_factor_score'] = bookReview_factor_score1

"""## Report factor score for new data"""

report_factor_score = []
train_news['report_factor_score'] = ""

for index, row in train_news.iterrows():
    title = row['headline_text']
    bow_vector = dictionary_Reports.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_Reports[bow_vector], key=lambda tup: -1*tup[1]): 
      # row['report_factor_score'] = score
      report_factor_score.append(score)
      # row['report_factor_topic'] = lda_model_Reports.print_topic(index, 5)
      break;

train_news['report_factor_score'] =report_factor_score

train_news.head(5)

report_factor_score1 = []
test_news['report_factor_score'] = ""

for index, row in test_news.iterrows():
    title = row['headline_text']
    bow_vector = dictionary_Reports.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_Reports[bow_vector], key=lambda tup: -1*tup[1]): 
      # row['report_factor_score'] = score
      report_factor_score1.append(score)
      # row['report_factor_topic'] = lda_model_Reports.print_topic(index, 5)
      break;

test_news['report_factor_score'] =report_factor_score1

"""Save dataset to drive"""

path = '/content/drive/MyDrive/ML-Spring-2022/TheTrio/Sprint3NLP/trainFactorScore.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  train_news.to_csv(f)

"""save test dataset"""

path = '/content/drive/MyDrive/ML-Spring-2022/TheTrio/Sprint3NLP/testFactorScore.csv'

with open(path, 'w', encoding = 'utf-8-sig') as f:
  test_news.to_csv(f)

"""So above dataframe gives the score for microfactors for given headline text.   
The score for each microfactor gives microfactor coverage in the text
"""

train_news.head(5)

"""##save model"""

class NewsCoverage:
  @classmethod
  def get_BookReview_score(self, text):
    score_business=0
    bow_vector = dictionary_BookReviews.doc2bow(preprocess(text))
    for index, score in sorted(lda_model_BookReviews[bow_vector], key=lambda tup: -1*tup[1]):
      score_business = score
      break;
    return score_business

  @classmethod
  def get_Report_score(self, text):
    score_report=0
    bow_vector = dictionary_Reports.doc2bow(preprocess(text))
    for index, score in sorted(lda_model_Reports[bow_vector], key=lambda tup: -1*tup[1]): 
      score_report=score
      break;
    return score_report

  @classmethod
  def get_business_score(self, text):
    score_business=0
    bow_vector = dictionary_business.doc2bow(preprocess(title))
    for index, score in sorted(lda_model_business[bow_vector], key=lambda tup: -1*tup[1]):
      score_business = score
      break;
    return score_business

  @classmethod
  def get_Broadcast_score(self, text):
    score_broadcast=0
    bow_vector = dictionary_broadcast.doc2bow(preprocess(text))
    for index, score in sorted(lda_model_broadcast[bow_vector], key=lambda tup: -1*tup[1]): 
      score_broadcast = score
      break;
    return score_broadcast

  @classmethod
  def getAvergaescore(self,text):

    average_score = (self.get_BookReview_score(text) + self.get_Report_score(text) + self.get_business_score(text) + self.get_Broadcast_score(text))/4
    return average_score

"""##pickle model"""

import pickle

NewsCoverageModel = NewsCoverage()
NewsCoverageModel_pickle = pickle.dumps(NewsCoverageModel)
with open('/content/drive/MyDrive/ML-Spring-2022/TheTrio/Sprint3NLP/NewsCoverageModelPickle.pickle', 'wb') as f:
    pickle.dump(NewsCoverageModel, f)

unseen_document = "Country music legend Naomi Judd, one half of The Judds, dies age 76"

pickel_model = pickle.load(open('/content/drive/MyDrive/ML-Spring-2022/TheTrio/Sprint3NLP/NewsCoverageModelPickle.pickle', 'rb'))
print(pickel_model.get_Broadcast_score(unseen_document))

"""#Scrape some article and get score for each microfactor"""

from bs4 import BeautifulSoup
from bs4.element import Comment
import urllib.request
import nltk
from urllib.request import urlopen
import json
import logging
import re

def beautify_txt(text):
    final_data = (((text).replace(u'\xa0', u' ')).replace(r'\r',u'\n').replace(r'\n', ' '))
    return text

articles=["https://www.vashonhistory.com/Vashon%20History/Newspaaper/newspaper_1937.htm"
          # "https://www.vashonhistory.com/Vashon%20History/Newspaaper/newspaper_1936.htm",
          # "https://www.vashonhistory.com/Vashon%20History/Newspaaper/newspaper_1935.htm",
          # "https://www.vashonhistory.com/Vashon%20History/Newspaaper/newspaper_1934.htm"
          ]

articalName = ["article1", "article2","article3", "article4"]

nltk.download('punkt')

for x in range(len(articles)):
     p_string = ""
     html_source = urlopen(articles[x])

     soup = BeautifulSoup(html_source, 'html.parser')
     try:
      for div in soup.find_all('section', class_='section'):
          for li in div.find_all('li'):
              p_tag = li.find('p')
              p_string = p_string + p_tag.getText()
              print(p_tag.getText())
      
     except Exception as e:
        logging.error(e)
      
     with open('/content/drive/MyDrive/ML-Spring-2022/TheTrio/Harika/NLP-Assignments/Assignment-10-AlternusVera_Part1&2/dataset/' + articalName[x] + '.txt', 'w') as outfile:
        json.dump(p_string, outfile)

p_string

"""Reports score"""

article = p_string

# Data preprocessing step for the unseen document
bow_vector = dictionary_Reports.doc2bow(preprocess(article))

for index, score in sorted(lda_model_Reports[bow_vector], key=lambda tup: -1*tup[1]): 
    print("Score: {}\t Topic: {}".format(score, lda_model_Reports.print_topic(index, 5)))

"""Book Review score"""

article = p_string

# Data preprocessing step for the unseen document
bow_vector = dictionary_BookReviews.doc2bow(preprocess(article))

for index, score in sorted(lda_model_BookReviews[bow_vector], key=lambda tup: -1*tup[1]): 
    print("Score: {}\t Topic: {}".format(score, lda_model_BookReviews.print_topic(index, 5)))

"""Broadcast"""

article = p_string

# Data preprocessing step for the unseen document
bow_vector = dictionary_broadcast.doc2bow(preprocess(article))

for index, score in sorted(lda_model_broadcast[bow_vector], key=lambda tup: -1*tup[1]): 
    print("Score: {}\t Topic: {}".format(score, lda_model_broadcast.print_topic(index, 5)))

"""Business"""

article = p_string

# Data preprocessing step for the unseen document
bow_vector = dictionary_business.doc2bow(preprocess(article))

for index, score in sorted(lda_model_business[bow_vector], key=lambda tup: -1*tup[1]): 
    print("Score: {}\t Topic: {}".format(score, lda_model_business.print_topic(index, 5)))

"""Conclusion:
The article has more news covergae related to book reviews
"""